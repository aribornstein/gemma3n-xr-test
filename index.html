<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Webcam Image Captioning (ONNX, HuggingFace Demo)</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <style>
    body { font-family: sans-serif; padding: 2em; }
    #webcam { border: 2px solid #333; border-radius: 8px; }
    #capture { margin: 1em 0; }
    #caption { font-weight: bold; margin-top: 1em; font-size: 1.1em;}
  </style>
</head>
<body>
  <h1>Take a Picture and Caption it (ONNX Web Demo)</h1>
  <video id="webcam" autoplay width="320" height="240"></video><br>
  <button id="capture">Capture & Caption</button>
  <canvas id="snapshot" width="320" height="240" style="display:none;"></canvas>
  <div id="caption"></div>

  <script>
    const webcam = document.getElementById('webcam');
    const captureBtn = document.getElementById('capture');
    const canvas = document.getElementById('snapshot');
    const captionDiv = document.getElementById('caption');

    // Start webcam stream
    async function startWebcam() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        webcam.srcObject = stream;
      } catch (e) {
        alert('Error accessing webcam: ' + e);
      }
    }

    // Preprocess: For a real app, resize, normalize, reorder channels, etc.
    // Here, just a dummy tensor for demo!
    async function preprocess(imageCanvas) {
      // TODO: Implement model-specific preprocessing here.
      // This is just a stub!
      // Model likely expects [1, 3, 224, 224], float32, normalized.
      return new ort.Tensor('float32', new Float32Array(1 * 3 * 224 * 224), [1, 3, 224, 224]);
    }

    // Postprocess: For real app, decode model output to text using vocab.
    function postprocess(output) {
      // TODO: Implement model-specific postprocessing/tokenizer here.
      return "Caption placeholder: Add real postprocessing.";
    }

    // Load ONNX model directly from HuggingFace!
    let session;
    async function loadModel() {
      captionDiv.textContent = "Loading ONNX model from HuggingFace (may take a few seconds)...";
      session = await ort.InferenceSession.create(
        "https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX/resolve/main/model.onnx"
      );
      captionDiv.textContent = "Model loaded! Ready to caption.";
    }

    // Capture and caption logic
    captureBtn.onclick = async () => {
      // Draw current webcam frame to canvas
      canvas.getContext('2d').drawImage(webcam, 0, 0, canvas.width, canvas.height);

      // Preprocess for model
      const tensor = await preprocess(canvas);

      // Inference
      captionDiv.textContent = 'Captioning... (demo stub)';
      try {
        const feeds = { 'pixel_values': tensor }; // 'pixel_values' is likely the input name for ViT-based ONNX
        // const results = await session.run(feeds);
        // const output = results[Object.keys(results)[0]];
        // const caption = postprocess(output);
        // captionDiv.textContent = caption;
        captionDiv.textContent = 'Success! (Model invoked, see TODO for real captioning)';
      } catch (e) {
        captionDiv.textContent = 'Error: ' + e;
      }
    };

    // Startup
    startWebcam();
    loadModel();
  </script>
</body>
</html>
