<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>ONNX Webcam Captioning (With Prompt!)</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <style>
    body { font-family: sans-serif; padding: 2em; }
    #webcam { border: 2px solid #333; border-radius: 8px; }
    #capture { margin: 1em 0; }
    #caption { font-weight: bold; margin-top: 1em; font-size: 1.1em;}
  </style>
</head>
<body>
  <h1>Take a Picture and Caption it (ONNX, Prompt-aware!)</h1>
  <video id="webcam" autoplay width="320" height="240"></video><br>
  <button id="capture">Capture & Caption</button>
  <canvas id="snapshot" width="224" height="224" style="display:none;"></canvas>
  <div id="caption"></div>
  <script>
    const webcam = document.getElementById('webcam');
    const captureBtn = document.getElementById('capture');
    const canvas = document.getElementById('snapshot');
    const captionDiv = document.getElementById('caption');
    const imagenet_mean = [0.485, 0.456, 0.406];
    const imagenet_std = [0.229, 0.224, 0.225];
    let vocab = [];
    let tokenToId = {};
    let idToToken = {};

    // Download vocab
    async function loadVocab() {
      const res = await fetch("https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX/resolve/main/vocab.txt");
      const text = await res.text();
      vocab = text.split('\n').filter(Boolean);
      vocab.forEach((token, idx) => {
        tokenToId[token] = idx;
        idToToken[idx] = token;
      });
    }

    // Simple whitespace tokenizer (for demo) -- adjust as needed for real model!
    function tokenize(text) {
      // For a real model, use their tokenizer; here, just split and match vocab.
      // Many HF caption models use "<image> caption this image" as the prompt.
      // Add special tokens if present in vocab: [BOS], [EOS], etc.
      // Let's try with "<image> caption this image" or Italian if you prefer.
      const prompt = "<image> caption this image";
      // Split by space, match vocab
      const ids = prompt.split(' ').map(tok => tokenToId[tok] ?? tokenToId['[UNK]'] ?? 0);
      return ids;
    }

    // Preprocess the image
    function preprocess(imageCanvas) {
      const ctx = imageCanvas.getContext('2d');
      const imageData = ctx.getImageData(0, 0, 224, 224);
      const { data } = imageData;
      const floatData = new Float32Array(3 * 224 * 224);
      for (let y = 0; y < 224; y++) {
        for (let x = 0; x < 224; x++) {
          const idx = (y * 224 + x) * 4;
          let r = data[idx] / 255;
          let g = data[idx + 1] / 255;
          let b = data[idx + 2] / 255;
          r = (r - imagenet_mean[0]) / imagenet_std[0];
          g = (g - imagenet_mean[1]) / imagenet_std[1];
          b = (b - imagenet_mean[2]) / imagenet_std[2];
          floatData[0 * 224 * 224 + y * 224 + x] = r;
          floatData[1 * 224 * 224 + y * 224 + x] = g;
          floatData[2 * 224 * 224 + y * 224 + x] = b;
        }
      }
      return new ort.Tensor('float32', floatData, [1, 3, 224, 224]);
    }

    // Decode token IDs to string
    function decodeTokenIds(tokenIds) {
      return tokenIds.map(id => idToToken[id] || '').join(' ').replace(/ ##/g, '').replace(/\s\s+/g, ' ').trim();
    }

    // Main logic
    let session;
    async function startWebcam() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        webcam.srcObject = stream;
      } catch (e) {
        alert('Error accessing webcam: ' + e);
      }
    }

    async function loadModel() {
      captionDiv.textContent = "Loading model and vocab...";
      session = await ort.InferenceSession.create(
        "https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX/resolve/main/model.onnx"
      );
      await loadVocab();
      captionDiv.textContent = "Model and vocab loaded! Ready.";
    }

    // For real models, you would autoregressively generate tokens until <eos>
    // This demo just runs one step and decodes the first output.
    captureBtn.onclick = async () => {
      canvas.getContext('2d').drawImage(webcam, 0, 0, 224, 224);
      const pixelTensor = preprocess(canvas);
      const inputIds = tokenize("<image> caption this image");
      // input_ids shape: [1, length]
      const inputTensor = new ort.Tensor('int64', Int32Array.from(inputIds), [1, inputIds.length]);

      captionDiv.textContent = 'Captioning...';
      try {
        // If the model needs additional inputs, check the model card!
        const feeds = {
          'pixel_values': pixelTensor,
          'input_ids': inputTensor
        };
        const results = await session.run(feeds);
        // Output is likely 'logits' or 'output_ids' (check actual model for output name)
        // Try common keys:
        let output;
        if (results.output_ids) {
          output = results.output_ids;
        } else if (results.logits) {
          output = results.logits;
        } else {
          output = results[Object.keys(results)[0]];
        }
        // Usually shape [1, seq_len]
        const outIds = Array.from(output.data).filter(id => id !== 0 && id !== 1);
        const caption = decodeTokenIds(outIds);
        captionDiv.textContent = caption || "[No caption, check console for details]";
      } catch (e) {
        captionDiv.textContent = 'Error: ' + e;
        console.error(e);
      }
    };

    startWebcam();
    loadModel();
  </script>
</body>
</html>
